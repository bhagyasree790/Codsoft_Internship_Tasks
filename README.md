# ğŸš¢ Titanic Survival Prediction - Task 1

## ğŸ§ Objective
To explore and understand the Titanic dataset, perform necessary data cleaning and preprocessing, and build a **classification model** using the **K-Nearest Neighbors (KNN)** algorithm to predict whether a passenger survived or not.

## ğŸ“Š Dataset
- **Source**: Titanic Dataset (commonly from [Kaggle Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic))
- Contains information like passenger demographics, ticket class, fare, etc.

## ğŸ›  Tools Used
- **Platform**: Google Colab  
- **Environment**: Anaconda  
- **Language**: Python  

## ğŸ“š Libraries Used
- `pandas` â€“ Data manipulation & analysis  
- `numpy` â€“ Numerical operations  
- `matplotlib` â€“ Data visualization  
- `seaborn` â€“ Statistical data visualization  

## ğŸ’¡ Machine Learning Algorithm
- **K-Nearest Neighbors (KNN)**  
  Used to classify passengers based on similarity in feature space.

---
---


# ğŸ¤– Task-2: Credit Card Fraud Detection

## ğŸ§ Objective
The goal of this project is to develop a robust machine learning model capable of identifying **fraudulent credit card transactions**. The major steps involve:

- ğŸ”„ **Data Preprocessing and Normalization**  
- âš–ï¸ **Handling Class Imbalance**  
- âœ‚ï¸ **Dataset Splitting**  
- ğŸ§  **Model Selection and Training**  
- ğŸ“Š **Performance Evaluation**

The aim is to accurately classify transactions as either **fraudulent** or **genuine**, even with highly imbalanced data.

---

## ğŸ“Š Dataset
- **Name**: Credit Card Fraud Detection Dataset  
- **Source**: [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)  
- **Details**: Contains transactions made by European cardholders in September 2013, with 492 frauds out of 284,807 transactions.

---

## ğŸ›  Tools Used
- **Platform**: Google Colab  
- **Environment**: Anaconda  
- **Language**: Python  

---

## ğŸ“š Libraries Used
- `pandas` â€“ Data manipulation  
- `numpy` â€“ Numerical computing  
- `matplotlib` & `seaborn` â€“ Data visualization  
- `sklearn` â€“ Machine learning models and evaluation metrics  

---

## ğŸ’¡ Machine Learning Algorithms
- **Logistic Regression**
- **K-Nearest Neighbors (KNN)**
- **Isolation Forest**
- **Local Outlier Factor (LOF)**

Each model is trained and evaluated to compare performance, especially under imbalanced conditions.

---    
---


# ğŸ“‘ Task-3: Sales Prediction

## ğŸ¤“ Objective
This project aims to build a **concise and effective sales prediction model** using machine learning techniques in Python. It involves:

- ğŸ“ˆ Collecting and exploring sales-related data  
- ğŸ§¹ Performing data preprocessing to ensure quality  
- ğŸ§  Training models to predict future sales based on advertising expenditure and audience segmentation  
- ğŸ§© Integrating the trained model for **business decision-making**, **ad strategy optimization**, and **efficient resource allocation**

---

## ğŸ“Š Dataset
- **Name**: Sales Prediction Dataset  
- **Type**: Tabular dataset containing advertising budgets and corresponding sales  
- **Focus**: Simple Linear Regression scenario (e.g., TV Ad Spend vs Sales)  
- **Source**: Custom/pre-loaded in ML tutorials (e.g., from CSV)

---

## ğŸ›  Tools Used
- **Platform**: Google Colab  
- **Environment**: Anaconda  
- **Language**: Python  

---

## ğŸ“š Libraries Used
- `pandas` â€“ Data loading and preprocessing  
- `numpy` â€“ Numerical operations  
- `matplotlib` & `seaborn` â€“ Data visualization  
- `sklearn` â€“ Model building and evaluation  

---

## ğŸ’¡ Machine Learning Algorithms
- **Linear Regression**  
- **Decision Trees**  
- **K-Nearest Neighbors (KNN)**  
- **Support Vector Machines (SVM)**  

All models are trained, tested, and compared to find the most suitable one for real-world sales forecasting.

---

## ğŸš€ Workflow
1. **Exploratory Data Analysis (EDA)** â€“ Understand distribution and correlations.
2. **Preprocessing** â€“ Normalize/scale features and handle missing data.
3. **Handling Imbalance** â€“ Techniques like undersampling, oversampling, SMOTE.
4. **Model Training** â€“ Train models like Logistic Regression, KNN, Isolation Forest, LOF.
5. **Evaluation** â€“ Accuracy, Precision, Recall, F1-Score, AUC-ROC.

---

## ğŸ“ˆ Results
Achieved a recall of **XX%** and precision of **YY%** on detecting fraudulent transactions using the best-performing model. *(Update after testing)*

---

## ğŸ™Œ Acknowledgements
Dataset provided by Codsoft

---


